---
title: "Weekly Summary Template"
author: "Author Name"
title-block-banner: true
title-block-style: default
toc: true
format: html
# format: pdf
---

---

## Tuesday, Jan 17

::: {.callout-important}
## TIL

Include a _very brief_ summary of what you learnt in this class here. 

Today, I learnt the following concepts in class:

1. Item 1
1. Item 2
1. Item 3
:::

Provide more concrete details here. You can also use footenotes[^footnote] if you like

```{R results='hide'}
library(dplyr) 
library(purrr) 
library(glmnet)
library(torch)
library(ISLR2)
library(tidyr) 
library(readr) 
library(caret)
library(mlbench)
library(nnet)
library(class)
library(rpart)
library(e1071) 
library(luz) 
library(torchvision)
```



Using the breast cancer data set we looked at a neural network with 3 hidden layers. In order for it to work we needed to specify 4 parameters: p,q1,q2,q3.

Then we fit the neural net using Luz. It was like this:
```{R
fit_nn <- NNet %>%
  
  setup(
    loss = nn_bce_loss(),
    optimizer = optim_rmsprop(),
    metrics = list(
      luz_metric_accuracy()
    )
  )

  set_hparams(p=...,q1=50,q2=30,q3=20) %>%
  set_opt_hparams(lr=0.01)
  
  fit(
    data = list(
      df_train %>%
       select(-y) %>%
       as.matrix,
      df_train %>%
        select(y) %>%
        as.matrix
    ),
    valid_data = list(
      df_test %>%
        select(-y) %>%
        as.matrix,
      df_test %>%
        select(y) %>%
        as.matrix,
    ),
    epochs = 50,
    verbose = TRUE
  )
```

Luz expects the data to be input as a list. This is important for the `fit` part of the code above. In this list we need to specify our X and Y. We select every variable but the response in one list and then just the response variable in an other. Don't forget to use `as.matrix()`.

Also its very useful because it doesn't matter what the data set looks like as long as the response variable is labeled `y` the code will work. Shown by the switch of the df from the breast cancer data set to the titanic data set.

We can now make predictions on this model using the predict function.

If a model has 0 intercept then when the intercept is 0 then everything else is also 0. This can be useful at times. For example if modeling horsepower to car price then it makes sense that if the horsepower is 0 then so should the price.

In this case the neural net did worse than logistic regression but the good part is that we can change some parameters like the learning rate that will give us better results. Also these were small data sets but if we were to do this with bigger ones then the neural nets would outperform the logistic regression models in a more noticable way.

#### Dataloaders

```{R}
dir <- "./mnist"

mnist_dataset2 <- torch::dataset(
  inherit = mnist_dataset,
  .getitem = function(i) {
    output <- super$.getitem(i)
    output$y <- output$x
    output
  }
)

train_ds <- mnist_dataset2(
  dir,
  download = TRUE,
  transform = transform
)

test_ds <- mnist_dataset2(
  dir,
  download = TRUE,
  train = FALSE,
  transform = transform
)
```

```{R}
typeof(train_ds)
```
This means that this automatically loads a bunch of different objects into our environment tab in Rstudio.

```{R}
length(train_ds)
```
So this means we have 60000 rows in our data set

```{R}
train_ds$data[42000, ,]
```
this is an image where the numbers are the intensity of the light of the pixels

```{R}
options(repr.plot.width=10,repr.plot.height=10)

i <- sample(1:length(train_ds), 1)
i <- 42000
x <- train_ds$data[i, ,] %>% t

image(x[1:28, 28:1], useRaster= TRUE, axes = FALSE, col = grey.colors(1000), main = train_ds$targets[i]-1)
```


_(the mnist data set is a part of torch so we dont really have to do the first part)_
It allows us to make data objects that have dataLoaders
 - Dataloaders are a key component in the machine learning pipeline
 - They handle loading and preprocessing data in a efficient way for training and evaluating models
 - They make it easy to work with large datasets by loading the data in smaller chunks, called batches and then applying transformation.
 
  **Why use them?**
 
 * Efficient memory management: loading data in smaller chunks which reduces memory usage.
 * Parallelism: It supports asynchronous data loading for faster processing
 * Preprocessing: apply data transformations on the fly during training and evaluation
 * Flexibility: easily switch between different data sets or preprocessing steps
 * Standardization: consistent data format across various machine learning projects.

### Image classification
The data set used above is scanned numbers and represented in 28x28 matrices where the value of each cell is the light value for each pixel. It also contains the right answer for what number the image represents.

```{R}
train_dl <- dataloader(train_ds, batch_size = 2000, shuffle = TRUE)
test_dl <- dataloader(test_ds, batch_size = 2000)
```



## Thursday, Jan 19



::: {.callout-important}
## TIL

Include a _very brief_ summary of what you learnt in this class here. 

Today, I learnt the following concepts in class:

1. Image Classification
1. Unsupervised learning
1. 
:::


_p has to be 28*28 because thats how big the matrices are and q3 has to be 10 because there are 10 classes (0,1,2,3,4,5,6,7,8,9)_
```{R}
NNet_10 <- nn_module(
  initialize = function(p, q1, q2, q3, o) {
    self$hidden1 <- nn_linear(p, q1)
    self$hidden2 <- nn_linear(q1, q2)
    self$hidden3 <- nn_linear(q2, q3)
    self$OUTPUT <- nn_linear(q3, o)
    self$activation <- nn_relu()
  },
  forward = function(x) {
    x %>%
      self$hidden1() %>%
      self$activation() %>%
      self$hidden2() %>%
      self$activation() %>%
      self$hidden3() %>%
      self$activation() %>%
      self$OUTPUT()
  }
)
```

_Now setting it up in Luz_
```{R
fit_nn <- NNet_10 %>%
    #
    # Setup the model
    #
    setup(
        loss = nn_cross_entropy_loss(),
        optimizer = optim_adam,
        metrics = list(
            luz_metric_accuracy()
        )
    ) %>%
    #
    # Set the hyperparameters
    #
    set_hparams(p=28*28, q1=256, q2=128, q3=64, o=10) %>% 
    #
    # Fit the model
    #
    fit(
        epochs = 10,
        data = train_dl,
        # valid_data = test_dl,
        verbose=TRUE
    )
```

Because there are 60,000 observations and 2,000 batches there will be 30 steps.

```{R
NN10_preds <- fit_nn %>% 
  predict(test_ds) %>% 
  torch_argmax(dim = 2) %>%
  as_array()
```

The model gives us a list of probabilities (in this case 10 items in the list since we have 10 classes) and we have to select the highest one. To do that we use the function `torch_argmex()`.

We can then create a confusion matrix with all the predicted values for the numbers and their target values. By looking at this matrix we can tell that it confuses 4 for 9 and 8 for 3 which makes sense give the similarity of the symbols.

### Supervised leaning
For most of this course so far we have focused on supervised learning where we have access to labelled data (we are given access to the covariates and the responses). And using this data our goals were

- using x to predict y
- understand how each x influences the response y

### Unsupervised learning 
In unsupervised We don't have access to the labled data. our goal is to

- understand the relationship between the Xs
  - __dimension reduction__ is when we try to discover subgroups of variables that behave similarly 
  -
  
#### Why unsupervised learning?
Unsupervised learning is easier to obtain unlabeled data as opposed to labeled data. (also someone had to label all those numbers that we used for image classification)

Unsupervised problems are more prevalent in nature so we encounter them more often in data science.

### Principal Component Analysis (PCA)

The objective of PCA is compressing all of the information of the data set into just 2 dimensions

It does this by trying to create variables $(Z_1, Z_2, Z_q)$ for $q < p$ such that:

1. $q << p$
2.$(Z_1, Z_2, Z_q)$ contains roughly the same information as $(X_1, X_2, X_q)$

__What does "the same amount of information mean??"__
It means that it has the same amount of variance.

__Step 1__
The first principal component $Z_1$ is the normalized linear combination of the features:

$Z_1 = v_11X_1 + v_21X_2 + \dots v_p1X_p$

such that:
 - $V_2$ and $V_1$ are orthogonal (the dot product of $V_2$ and $V_1$ is equal to 0)
 - $Z_1$ has the largest possible variance
 - the sum of $v^2_p  = 1$




[^footnote]: You can include some footnotes here